{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOnrkikqTOGZAgrT26VXvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Transformers/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "[kaggle tutorial link](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook)\n",
        "\n"
      ],
      "metadata": {
        "id": "FKr4QDPjHguA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Import Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mnsYzhidHYf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torchtext\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BXLifcJHX6G",
        "outputId": "d26d7ec9-a9b3-4f32-f980-765da34a0222"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic components"
      ],
      "metadata": {
        "id": "JHra7os-IbcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Word Embeddings\n",
        "\n",
        "First of all we need to convert each word in the input sequence to an embedding vector. Embedding vectors will create a more semantic representation of each word.\n",
        "\n",
        "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. These marix will be learned on training and during inference each word will be mapped to corresponding 512 d vector. Suppose we have batch size of 32 and sequence length of 10(10 words). The the output will be 32x10x512."
      ],
      "metadata": {
        "id": "YjqniYu4IdrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim):\n",
        "    super(Embedding,self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.embed(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "gwlXhAmHHX2N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "\n",
        "Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 and it is added with the correspondng positional vector which is of dimension 1 x 512 to get 1 x 512 dim out for each word/token.\n",
        "\n",
        "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 32 x 10 x 512. Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both"
      ],
      "metadata": {
        "id": "yucBO_-0JbDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register buffer in Pytorch ->\n",
        "# If you have parameters in your model, which should be saved and restored in the state_dict,\n",
        "# but not trained by the optimizer, you should register them as buffers.\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, max_seq_length, embed_model_dim):\n",
        "    super(PositionalEmbedding,self).__init__()\n",
        "    self.embed_dim = embed_model_dim\n",
        "    "
      ],
      "metadata": {
        "id": "LE0_6-rHHXy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qq-BtnA-HXny"
      }
    }
  ]
}